{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from abc import ABC, abstractmethod\n",
    "from jpype import *\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import participant_ids, raw_data_path, find_trials, import_soa_rating_data, import_emg_data, onset_detection, extract_onset_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate jitd jar file\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append(\n",
    "    \"../\"\n",
    ")\n",
    "\n",
    "\n",
    "from TE import TransferEntropyCalculator_continuous_kraskov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20250604 D:\\SynologyDrive\\Drive-Acer\\DeepWen\\deepwen\\home\\acercyc\\Projects\\Drum\\data\\raw_20250911\\SoA\\20250604\\morita_0604_soa.csv 6\n",
      "20250606 D:\\SynologyDrive\\Drive-Acer\\DeepWen\\deepwen\\home\\acercyc\\Projects\\Drum\\data\\raw_20250911\\SoA\\20250606\\morita_0606_soa.csv 8\n",
      "20250611 D:\\SynologyDrive\\Drive-Acer\\DeepWen\\deepwen\\home\\acercyc\\Projects\\Drum\\data\\raw_20250911\\SoA\\20250611\\morita_0611_soa.csv 10\n",
      "20250613 D:\\SynologyDrive\\Drive-Acer\\DeepWen\\deepwen\\home\\acercyc\\Projects\\Drum\\data\\raw_20250911\\SoA\\20250613\\morita_0613_soa.csv 11\n",
      "20250618 D:\\SynologyDrive\\Drive-Acer\\DeepWen\\deepwen\\home\\acercyc\\Projects\\Drum\\data\\raw_20250911\\SoA\\20250618\\morita_0618_soa.csv 10\n",
      "20250620 D:\\SynologyDrive\\Drive-Acer\\DeepWen\\deepwen\\home\\acercyc\\Projects\\Drum\\data\\raw_20250911\\SoA\\20250620\\morita_0620_soa.csv 9\n"
     ]
    }
   ],
   "source": [
    "# Find files in folders and subfolders\n",
    "from pair_data_paths import build_grouped, default_base_dir\n",
    "base = default_base_dir()\n",
    "grouped = build_grouped(base / \"NI\", base / \"SoA\")  # list of SoAToNIs\n",
    "for g in grouped:\n",
    "    print(g.date, g.soa_csv, len(g.ni_files))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TE was optimised via Ragwitz criteria: k=6, k_tau=3, l=6, l_tau=6\n",
      "Transfer Entropy result: 0.02901197142154155\n"
     ]
    }
   ],
   "source": [
    "# convolue\n",
    "def convolve_with_exponential_kernel(data, decay_rate, kernel_size):\n",
    "    \"\"\"\n",
    "    Convolve a column with an exponential kernel for smoothing.\n",
    "\n",
    "    Parameters:\n",
    "    - data (np.array): The data to be smoothed.\n",
    "    - decay_rate (float): The decay rate (lambda) for the exponential kernel.\n",
    "\n",
    "    Returns:\n",
    "    - np.array: The convolved data.\n",
    "    \"\"\"\n",
    "    kernel = np.exp(-decay_rate * np.linspace(0, 1, kernel_size))\n",
    "    # kernel /= kernel.sum()  # Normalize kernel to maintain scale\n",
    "\n",
    "    data_cov = np.convolve(data, kernel, mode=\"same\")\n",
    "    return data_cov\n",
    "\n",
    "\n",
    "def impulse_response_convolution(\n",
    "    data, peak_amplitude=1.0, decay_rate=0.001, response_length=200\n",
    "):\n",
    "    \"\"\"\n",
    "    Applies an impulse response function with exponential decay using convolution.\n",
    "\n",
    "    Parameters:\n",
    "        time_series (numpy array): The input time series (e.g., binary impulses).\n",
    "        peak_amplitude (float): The peak height of the response.\n",
    "        decay_rate (float): The exponential decay rate (higher means faster decay).\n",
    "        response_length (int): The length of the impulse response function.\n",
    "\n",
    "    Returns:\n",
    "        numpy array: The output signal with impulse responses applied.\n",
    "    \"\"\"\n",
    "    # Generate the impulse response kernel\n",
    "    data = np.abs(data)\n",
    "    t = np.arange(response_length)\n",
    "    impulse_response_kernel = peak_amplitude * np.exp(-decay_rate * t)\n",
    "\n",
    "    # Apply convolution\n",
    "    output_signal = convolve(data, impulse_response_kernel, mode=\"full\")[: len(data)]\n",
    "\n",
    "    # Normalize the output signal\n",
    "    output_signal /= np.max(output_signal)\n",
    "\n",
    "    return output_signal\n",
    "\n",
    "\n",
    "def compute_TE_from_input_file(file_path):\n",
    "    # Load data\n",
    "    data = pd.read_csv(file_path)\n",
    "    data = pd.read_csv(file_path, sep=\"\\t\")\n",
    "\n",
    "    \n",
    "    # Extract signals\n",
    "    target = data['ACC_HIHAT[V]'].values\n",
    "    source = data['Correct_Timing_Signal[V]'].values\n",
    "    t = data['Time[s]'].values\n",
    "    \n",
    "    # Apply convolution smoothing\n",
    "    decay_rate = 0.001\n",
    "    response_length = 1000\n",
    "    target_cov = impulse_response_convolution(\n",
    "        target, peak_amplitude=1, decay_rate=decay_rate, response_length=response_length\n",
    "    )\n",
    "\n",
    "    source_cov = impulse_response_convolution(\n",
    "        source, peak_amplitude=1, decay_rate=decay_rate, response_length=response_length\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Find signal boundaries (non-zero regions)\n",
    "    onsets_array, onset_idx = onset_detection(source, threshold=0.1, minimal_interval=1000)\n",
    "    idx_start = onset_idx[0]\n",
    "    idx_end = onset_idx[-1]\n",
    "    \n",
    "    # Truncate the signals\n",
    "    target_ = target_cov[idx_start:idx_end]\n",
    "    source_ = source_cov[idx_start:idx_end]\n",
    "    t_ = t[idx_start:idx_end]\n",
    "    \n",
    "    # Down sample the signals\n",
    "    original_sampling_rate = 10000\n",
    "    down_sampling_rate = 100  # Hz\n",
    "    down_sample_point = int(original_sampling_rate / down_sampling_rate)\n",
    "    target_ = target_[::down_sample_point]\n",
    "    source_ = source_[::down_sample_point]\n",
    "    t_ = t_[::down_sample_point]\n",
    "    \n",
    "    # Compute Transfer Entropy\n",
    "    te_calc = TransferEntropyCalculator_continuous_kraskov()\n",
    "    te_result = te_calc.compute_TE(source_, target_, isPrintEstimation=True)\n",
    "    \n",
    "    return te_result\n",
    "\n",
    "# Test the function with the current file\n",
    "file_path = \"../../data/raw_20250911/NI/20250604/1_20250604_135334_ni.txt\"\n",
    "te_result = compute_TE_from_input_file(file_path)\n",
    "print(f\"Transfer Entropy result: {te_result}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
